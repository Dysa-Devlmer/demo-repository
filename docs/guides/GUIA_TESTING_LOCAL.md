# üß™ Gu√≠a de Testing Local - ChatBotDysa

**Fecha:** 2025-11-01
**Prop√≥sito:** Probar el sistema localmente antes de llevarlo a restaurantes

---

## üìã Tabla de Contenidos

1. [Configuraci√≥n R√°pida](#configuraci√≥n-r√°pida)
2. [3 Formas de Usar el Chatbot IA](#3-formas-de-usar-el-chatbot-ia)
3. [Testing del Sistema Completo](#testing-del-sistema-completo)
4. [Scripts de Prueba R√°pida](#scripts-de-prueba-r√°pida)
5. [Troubleshooting](#troubleshooting)

---

## üöÄ Configuraci√≥n R√°pida

### Opci√≥n A: Solo Backend + Ollama (Recomendado para Testing)

```bash
# 1. Instalar Ollama (si no lo tienes)
# macOS:
brew install ollama

# Linux:
curl -fsSL https://ollama.com/install.sh | sh

# Windows:
# Descargar de https://ollama.com/download

# 2. Iniciar Ollama
ollama serve

# 3. En otra terminal, descargar el modelo
ollama pull phi3:mini

# 4. Configurar Backend
cd /Users/devlmer/ChatBotDysa/apps/backend

# 5. Copiar .env.example
cp .env.example .env

# 6. Editar .env (valores m√≠nimos para testing)
nano .env
```

**Configuraci√≥n m√≠nima de .env:**
```bash
# Database (usar PostgreSQL local o Docker)
DATABASE_HOST=localhost
DATABASE_PORT=5432
DATABASE_USER=postgres
DATABASE_PASSWORD=supersecret
DATABASE_NAME=chatbotdysa

# JWT
JWT_SECRET=your-super-secret-jwt-key-for-testing
JWT_EXPIRES_IN=24h

# Ollama (IA Local)
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=phi3:mini

# OpenAI (OPCIONAL - si no lo configuras, usa solo Ollama)
# OPENAI_API_KEY=sk-...
# OPENAI_MODEL=gpt-4o-mini

# URLs
APP_URL=http://localhost:3004
ADMIN_URL=http://localhost:7001
API_URL=http://localhost:8005

# CORS
CORS_ORIGINS=http://localhost:3004,http://localhost:7001

# Features
ENABLE_SWAGGER=true
ENABLE_RATE_LIMITING=false
NODE_ENV=development
PORT=8005
```

```bash
# 7. Instalar dependencias
npm install

# 8. Iniciar PostgreSQL (Docker)
docker run -d \
  --name chatbotdysa-postgres \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_PASSWORD=supersecret \
  -e POSTGRES_DB=chatbotdysa \
  -p 5432:5432 \
  postgres:16

# 9. Ejecutar migraciones
npm run migration:run

# 10. Cargar datos de prueba (opcional)
npm run seed

# 11. Iniciar backend
npm run start:dev

# ‚úÖ Backend corriendo en http://localhost:8005
# ‚úÖ API Docs en http://localhost:8005/api
```

### Opci√≥n B: Sistema Completo (Backend + Admin Panel + Landing)

```bash
# Terminal 1: Backend (arriba)
cd apps/backend
npm run start:dev

# Terminal 2: Admin Panel
cd apps/admin-panel
npm install
cp .env.example .env.local

# Editar .env.local
nano .env.local
```

**.env.local para Admin Panel:**
```bash
NEXT_PUBLIC_API_URL=http://localhost:8005
NEXTAUTH_URL=http://localhost:7001
NEXTAUTH_SECRET=your-nextauth-secret-key
```

```bash
npm run dev
# ‚úÖ Admin Panel en http://localhost:7001
```

```bash
# Terminal 3: Landing Page (opcional)
cd apps/landing-page
npm install
cp .env.example .env.local
npm run dev
# ‚úÖ Landing en http://localhost:3004
```

---

## ü§ñ 3 Formas de Usar el Chatbot IA

### 1Ô∏è‚É£ CON API (Modo H√≠brido - RECOMENDADO)

**C√≥mo funciona:**
- Tu frontend ‚Üí Backend API ‚Üí HybridAIService
- El sistema intenta: **OpenAI** (si configurado) ‚Üí **Ollama** ‚Üí **Respuestas pre-programadas**
- ‚úÖ Mejor para producci√≥n
- ‚úÖ Incluye cach√© de respuestas
- ‚úÖ Fallback autom√°tico
- ‚úÖ Control centralizado

**Configuraci√≥n:**

**Solo Ollama (Sin OpenAI):**
```bash
# En apps/backend/.env
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=phi3:mini

# NO configurar OPENAI_API_KEY
# El sistema usar√° solo Ollama
```

**Con OpenAI + Ollama (Fallback):**
```bash
# En apps/backend/.env
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=phi3:mini

# Configurar OpenAI (opcional)
OPENAI_API_KEY=sk-proj-xxxxx
OPENAI_MODEL=gpt-4o-mini
```

**Probar con cURL:**
```bash
# 1. Login para obtener token
TOKEN=$(curl -s -X POST http://localhost:8005/auth/login \
  -H "Content-Type: application/json" \
  -d '{
    "email": "admin@zgamersa.com",
    "password": "Admin123!"
  }' | jq -r '.token')

# 2. Hacer una pregunta al chatbot
curl -X POST http://localhost:8005/ai/chat \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $TOKEN" \
  -d '{
    "message": "Hola, quisiera hacer una reserva para 4 personas",
    "customerName": "Juan P√©rez",
    "context": {
      "restaurantInfo": {
        "name": "Restaurante El Sabor",
        "address": "Calle Principal 123",
        "phone": "+56912345678",
        "hours": "Lunes a Domingo 12:00 - 23:00",
        "specialties": ["Parrillas", "Mariscos", "Pastas"]
      },
      "menuItems": [
        {
          "id": 1,
          "name": "Parrillada Premium",
          "price": 25000,
          "category": "Carnes",
          "description": "Mix de carnes selectas",
          "available": true
        },
        {
          "id": 2,
          "name": "Ceviche de Corvina",
          "price": 12000,
          "category": "Mariscos",
          "description": "Fresco del d√≠a",
          "available": true
        }
      ]
    }
  }'
```

**Probar desde Frontend (Admin Panel):**
```javascript
// En Admin Panel, crear p√°gina de prueba: apps/admin-panel/src/app/test-chat/page.tsx
const response = await fetch('http://localhost:8005/ai/chat', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'Authorization': `Bearer ${token}`,
  },
  body: JSON.stringify({
    message: "Hola, quisiera ver el men√∫",
    customerName: "Cliente Test",
    context: {
      restaurantInfo: {
        name: "Mi Restaurante",
        // ... resto del contexto
      }
    }
  })
});

const data = await response.json();
console.log(data.response); // Respuesta del AI
```

---

### 2Ô∏è‚É£ SIN API (Directo a Ollama)

**C√≥mo funciona:**
- Tu c√≥digo ‚Üí Ollama directamente (sin backend)
- ‚úÖ M√°s r√°pido (sin intermediarios)
- ‚úÖ √ötil para testing local
- ‚ùå Sin fallback autom√°tico
- ‚ùå Sin cach√©
- ‚ùå Sin autenticaci√≥n

**Instalaci√≥n del cliente Ollama:**
```bash
npm install ollama
```

**C√≥digo de ejemplo:**
```typescript
// test-ollama-direct.ts
import { Ollama } from 'ollama';

const ollama = new Ollama({
  host: 'http://localhost:11434'
});

async function chatDirectWithOllama() {
  const response = await ollama.chat({
    model: 'phi3:mini',
    messages: [
      {
        role: 'system',
        content: `Eres ChefBot Dysa, asistente del Restaurante El Sabor.

INFORMACI√ìN:
- Nombre: Restaurante El Sabor
- Horarios: Lunes a Domingo 12:00 - 23:00
- Especialidades: Parrillas, Mariscos, Pastas

MEN√ö:
- Parrillada Premium: $25.000
- Ceviche de Corvina: $12.000
- Pasta Carbonara: $8.500

Responde solo sobre el restaurante.`
      },
      {
        role: 'user',
        content: '¬øCu√°l es el plato m√°s caro?'
      }
    ],
    stream: false,
  });

  console.log('Respuesta:', response.message.content);
}

chatDirectWithOllama();
```

**Ejecutar:**
```bash
npx ts-node test-ollama-direct.ts
```

**Ventajas:**
- Sin necesidad de backend
- M√°s r√°pido para prototipos
- Control total del prompt

**Desventajas:**
- Sin integraci√≥n con la base de datos
- Sin sistema de fallback
- Tienes que manejar errores manualmente

---

### 3Ô∏è‚É£ PERSONALIZADO (Modificar Prompts y Comportamiento)

**Ubicaci√≥n de los prompts:**

**A. OllamaService (Solo Ollama)**
- Archivo: `/Users/devlmer/ChatBotDysa/apps/backend/src/modules/ai/ollama.service.ts`
- L√≠nea: 320-365
- M√©todo: `buildRestaurantSystemPrompt()`

**B. HybridAIService (Sistema H√≠brido)**
- Archivo: `/Users/devlmer/ChatBotDysa/apps/backend/src/modules/ai/hybrid-ai.service.ts`
- L√≠nea: 119-178
- M√©todo: `buildRestrictedSystemPrompt()`

**Personalizaci√≥n 1: Cambiar la Personalidad del Bot**

```typescript
// En ollama.service.ts l√≠nea 324
return `Eres ChefBot Dysa üë®‚Äçüç≥, el asistente inteligente de ${restaurantName}.

PERSONALIDAD:
- Muy formal y elegante (CAMBIO AQU√ç)
- Experto en alta cocina
- Usa lenguaje sofisticado
- Evita emojis (CAMBIO AQU√ç)

CAPACIDADES PRINCIPALES:
1. üìÖ Gestionar reservas
...
```

**Personalizaci√≥n 2: Agregar Nuevas Capacidades**

```typescript
// En hybrid-ai.service.ts l√≠nea 134
‚úÖ PUEDES AYUDAR CON:
- üçΩÔ∏è Consultar men√∫, precios, ingredientes, platos del d√≠a
- üìÖ Hacer, modificar o cancelar reservas
- üõµ Tomar pedidos para delivery o takeaway
- üéÇ Sugerir men√∫s para eventos especiales (NUEVO)
- üç∑ Recomendar maridajes de vino (NUEVO)
- ‚ÑπÔ∏è Informaci√≥n del restaurante
```

**Personalizaci√≥n 3: Cambiar Restricciones**

```typescript
// En hybrid-ai.service.ts l√≠nea 124
üö´ RESTRICCIONES ABSOLUTAS:
1. SOLO puedes hablar sobre ${restaurantName}
2. PUEDES dar consejos gastron√≥micos generales (CAMBIO AQU√ç)
3. NO respondas sobre competidores
4. SI te preguntan recetas, comparte solo las del restaurante
```

**Personalizaci√≥n 4: Cambiar Modelo de Ollama**

```bash
# En .env
OLLAMA_MODEL=llama3:8b  # M√°s inteligente pero m√°s lento
# OLLAMA_MODEL=phi3:mini  # M√°s r√°pido pero menos inteligente
# OLLAMA_MODEL=mistral   # Balance entre velocidad e inteligencia
```

**Descargar modelos adicionales:**
```bash
# Modelo peque√±o y r√°pido
ollama pull phi3:mini

# Modelo mediano (recomendado)
ollama pull llama3:8b

# Modelo grande (m√°s inteligente)
ollama pull llama3:70b

# Ver modelos instalados
ollama list
```

**Personalizaci√≥n 5: Ajustar Par√°metros de Generaci√≥n**

```typescript
// En ollama.service.ts l√≠nea 234
options: {
  temperature: 0.5,        // 0.0 = determinista, 1.0 = creativo
  top_k: 40,               // Limita opciones de palabras
  top_p: 0.9,              // N√∫cleo de probabilidad
  repeat_penalty: 1.2,     // Evita repetici√≥n (m√°s alto = menos repetici√≥n)
  num_ctx: 4096,           // Contexto (m√°s = m√°s memoria)
  num_predict: 200,        // Tokens m√°ximos de respuesta
}
```

**Personalizaci√≥n 6: Agregar Respuestas Pre-programadas (Fallback)**

```typescript
// En hybrid-ai.service.ts l√≠nea 257
private getFallbackResponse(userMessage: string, context: RestaurantContext): string {
  const lowerMessage = userMessage.toLowerCase();

  // AGREGAR NUEVA RESPUESTA PERSONALIZADA
  if (lowerMessage.includes('evento') || lowerMessage.includes('celebraci√≥n')) {
    return `¬°Perfecto para eventos! üéâ En ${restaurantName} organizamos:
    - Cumplea√±os
    - Aniversarios
    - Eventos corporativos
    - Cenas privadas

    ¬øPara cu√°ntas personas ser√≠a el evento?`;
  }

  // ... resto de respuestas
}
```

---

## üß™ Testing del Sistema Completo

### Test 1: Verificar que Ollama est√° Corriendo

```bash
# Verificar versi√≥n
curl http://localhost:11434/api/version

# Verificar modelos instalados
curl http://localhost:11434/api/tags

# Test desde el backend
curl http://localhost:8005/ai/health
```

### Test 2: Probar Diferentes Tipos de Mensajes

Crea un script de prueba:

```bash
# /Users/devlmer/ChatBotDysa/test-ai-scenarios.sh
#!/bin/bash

TOKEN="eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..." # Tu token de admin

BASE_URL="http://localhost:8005"

echo "üß™ Probando diferentes escenarios del chatbot..."

# Escenario 1: Saludo
echo -e "\n1Ô∏è‚É£ SALUDO"
curl -X POST $BASE_URL/ai/chat \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $TOKEN" \
  -d '{"message": "Hola, buenos d√≠as"}' | jq -r '.response'

# Escenario 2: Consulta de men√∫
echo -e "\n2Ô∏è‚É£ CONSULTA DE MEN√ö"
curl -X POST $BASE_URL/ai/chat \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $TOKEN" \
  -d '{
    "message": "¬øCu√°les son sus especialidades?",
    "context": {
      "restaurantInfo": {
        "name": "El Sabor Gourmet",
        "specialties": ["Parrillas Premium", "Mariscos Frescos", "Pastas Artesanales"]
      }
    }
  }' | jq -r '.response'

# Escenario 3: Reserva
echo -e "\n3Ô∏è‚É£ SOLICITUD DE RESERVA"
curl -X POST $BASE_URL/ai/chat \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $TOKEN" \
  -d '{
    "message": "Quiero hacer una reserva para 6 personas este s√°bado",
    "customerName": "Mar√≠a Gonz√°lez"
  }' | jq -r '.response'

# Escenario 4: Pregunta fuera de contexto
echo -e "\n4Ô∏è‚É£ PREGUNTA FUERA DE CONTEXTO"
curl -X POST $BASE_URL/ai/chat \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $TOKEN" \
  -d '{"message": "¬øQui√©n gan√≥ el mundial 2022?"}' | jq -r '.response'

# Escenario 5: Consulta con men√∫ real
echo -e "\n5Ô∏è‚É£ CONSULTA CON MEN√ö REAL"
curl -X POST $BASE_URL/ai/chat \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $TOKEN" \
  -d '{
    "message": "¬øCu√°l es el plato m√°s caro?",
    "context": {
      "menuItems": [
        {"name": "Ensalada C√©sar", "price": 8500, "category": "Entradas"},
        {"name": "Parrillada Premium", "price": 28000, "category": "Carnes"},
        {"name": "Ceviche", "price": 12000, "category": "Mariscos"}
      ]
    }
  }' | jq -r '.response'

echo -e "\n‚úÖ Tests completados"
```

```bash
chmod +x test-ai-scenarios.sh
./test-ai-scenarios.sh
```

### Test 3: Probar Performance

```bash
# test-ai-performance.sh
#!/bin/bash

echo "‚è±Ô∏è Probando performance del AI..."

for i in {1..10}
do
  echo "Request $i..."
  time curl -s -X POST http://localhost:8005/ai/chat \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $TOKEN" \
    -d '{"message": "Hola"}' > /dev/null
done
```

---

## üìä Scripts de Prueba R√°pida

### Script 1: Setup R√°pido

```bash
# quick-setup.sh
#!/bin/bash

echo "üöÄ Setup r√°pido de ChatBotDysa..."

# 1. Verificar Ollama
if ! command -v ollama &> /dev/null; then
    echo "‚ùå Ollama no est√° instalado. Instalando..."
    brew install ollama
fi

# 2. Iniciar Ollama (background)
echo "‚úÖ Iniciando Ollama..."
ollama serve &
sleep 3

# 3. Descargar modelo
echo "üì• Descargando modelo phi3:mini..."
ollama pull phi3:mini

# 4. Iniciar PostgreSQL (Docker)
echo "üêò Iniciando PostgreSQL..."
docker run -d \
  --name chatbotdysa-postgres \
  -e POSTGRES_PASSWORD=supersecret \
  -e POSTGRES_DB=chatbotdysa \
  -p 5432:5432 \
  postgres:16

# 5. Configurar backend
echo "‚öôÔ∏è Configurando backend..."
cd apps/backend
cp .env.example .env

# 6. Instalar dependencias
echo "üì¶ Instalando dependencias..."
npm install

# 7. Migraciones
echo "üóÑÔ∏è Ejecutando migraciones..."
npm run migration:run

# 8. Seeds
echo "üå± Cargando datos de prueba..."
npm run seed

echo "‚úÖ Setup completo. Ejecuta: npm run start:dev"
```

### Script 2: Test Completo del Sistema

```bash
# test-complete-system.sh
#!/bin/bash

echo "üß™ Testing completo del sistema..."

# Colores
GREEN='\033[0;32m'
RED='\033[0;31m'
NC='\033[0m'

# 1. Test Ollama
echo -e "\n1Ô∏è‚É£ Testing Ollama..."
if curl -s http://localhost:11434/api/version > /dev/null; then
    echo -e "${GREEN}‚úÖ Ollama OK${NC}"
else
    echo -e "${RED}‚ùå Ollama no responde${NC}"
fi

# 2. Test Backend
echo -e "\n2Ô∏è‚É£ Testing Backend..."
if curl -s http://localhost:8005/health > /dev/null; then
    echo -e "${GREEN}‚úÖ Backend OK${NC}"
else
    echo -e "${RED}‚ùå Backend no responde${NC}"
fi

# 3. Test PostgreSQL
echo -e "\n3Ô∏è‚É£ Testing PostgreSQL..."
if docker exec chatbotdysa-postgres pg_isready > /dev/null 2>&1; then
    echo -e "${GREEN}‚úÖ PostgreSQL OK${NC}"
else
    echo -e "${RED}‚ùå PostgreSQL no responde${NC}"
fi

# 4. Test AI Endpoint
echo -e "\n4Ô∏è‚É£ Testing AI Endpoint..."
AI_HEALTH=$(curl -s http://localhost:8005/ai/health | jq -r '.isRunning')
if [ "$AI_HEALTH" = "true" ]; then
    echo -e "${GREEN}‚úÖ AI Service OK${NC}"
else
    echo -e "${RED}‚ùå AI Service no funciona${NC}"
fi

# 5. Test Admin Panel (si est√° corriendo)
echo -e "\n5Ô∏è‚É£ Testing Admin Panel..."
if curl -s http://localhost:7001 > /dev/null; then
    echo -e "${GREEN}‚úÖ Admin Panel OK${NC}"
else
    echo -e "${RED}‚ö†Ô∏è  Admin Panel no est√° corriendo${NC}"
fi

echo -e "\nüéâ Tests completados"
```

---

## üêõ Troubleshooting

### Problema 1: Ollama no se conecta

```bash
# Error: ECONNREFUSED
# Soluci√≥n:

# 1. Verificar que Ollama est√° corriendo
ps aux | grep ollama

# 2. Si no est√° corriendo
ollama serve

# 3. Verificar puerto
lsof -i :11434

# 4. Verificar configuraci√≥n en .env
echo $OLLAMA_URL  # Debe ser http://localhost:11434
```

### Problema 2: Modelo no encontrado

```bash
# Error: Model phi3:mini not found
# Soluci√≥n:

# 1. Ver modelos instalados
ollama list

# 2. Descargar el modelo
ollama pull phi3:mini

# 3. Verificar que el nombre en .env coincide
# OLLAMA_MODEL=phi3:mini
```

### Problema 3: PostgreSQL no conecta

```bash
# Error: Connection refused to PostgreSQL
# Soluci√≥n:

# 1. Verificar que PostgreSQL est√° corriendo
docker ps | grep postgres

# 2. Si no est√°, iniciarlo
docker start chatbotdysa-postgres

# 3. Verificar conexi√≥n
psql -h localhost -U postgres -d chatbotdysa

# 4. Verificar credenciales en .env
DATABASE_HOST=localhost
DATABASE_PORT=5432
DATABASE_USER=postgres
DATABASE_PASSWORD=supersecret
```

### Problema 4: Respuestas muy lentas

```bash
# Soluci√≥n 1: Usar modelo m√°s peque√±o
ollama pull phi3:mini  # ~2GB, r√°pido
# En lugar de:
# ollama pull llama3:8b  # ~5GB, m√°s lento

# Soluci√≥n 2: Ajustar par√°metros
# En ollama.service.ts l√≠nea 239
num_ctx: 2048,      # Reducir contexto
num_predict: 100,   # Reducir tokens de respuesta
```

### Problema 5: OpenAI API Key inv√°lida

```bash
# Error: Invalid API key
# Soluci√≥n:

# 1. Verificar que la key es correcta
echo $OPENAI_API_KEY

# 2. Si no tienes OpenAI, desactivarlo
# En .env, NO configurar OPENAI_API_KEY
# El sistema usar√° solo Ollama

# 3. Verificar logs
tail -f apps/backend/logs/app.log
```

---

## üìù Checklist de Testing

Antes de llevar el sistema a un restaurante:

- [ ] Ollama corriendo y responde
- [ ] Modelo phi3:mini descargado
- [ ] Backend responde en /health
- [ ] PostgreSQL conectado
- [ ] Migraciones ejecutadas
- [ ] Seeds cargados (datos de prueba)
- [ ] AI endpoint /ai/chat responde
- [ ] Admin Panel carga correctamente
- [ ] Login funciona (admin@zgamersa.com)
- [ ] Probados 5+ escenarios diferentes
- [ ] Respuestas coherentes del chatbot
- [ ] Tiempo de respuesta < 5 segundos
- [ ] Fallback funciona si Ollama falla

---

## üéØ Pr√≥ximos Pasos

Una vez probado localmente:

1. **Configurar datos reales del restaurante**
   - Actualizar men√∫ en la base de datos
   - Configurar informaci√≥n del restaurante
   - Personalizar prompts del AI

2. **Entrenar el modelo** (opcional)
   - Fine-tuning con conversaciones reales
   - Ajustar prompts seg√∫n feedback

3. **Desplegar en producci√≥n**
   - Seguir gu√≠a: [COMO_DESPLEGAR.md](./COMO_DESPLEGAR.md)
   - Configurar dominio y SSL
   - Configurar backups

---

**¬øPreguntas?**
Consulta [DEPLOYMENT_GUIDE.md](./DEPLOYMENT_GUIDE.md) para despliegue en producci√≥n.

üéâ **¬°Listo para probar el sistema!**
