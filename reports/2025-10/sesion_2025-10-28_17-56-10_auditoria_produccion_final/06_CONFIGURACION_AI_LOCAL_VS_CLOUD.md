# ğŸ¤– CONFIGURACIÃ“N AI: LOCAL vs CLOUD

**ChatBotDysa Enterprise - Opciones de IA**
**Fecha:** 28 de Octubre de 2025

---

## ğŸ¯ ARQUITECTURA ACTUAL

### Sistema Principal: 100% LOCAL âœ…
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         TODO EN TU MAC (Docker)             â”‚
â”‚                                             â”‚
â”‚  âœ… Backend (NestJS)        â†’ Local        â”‚
â”‚  âœ… PostgreSQL              â†’ Local        â”‚
â”‚  âœ… Redis                   â†’ Local        â”‚
â”‚  âœ… Frontend                â†’ Local        â”‚
â”‚  âœ… Ollama AI (phi3:mini)   â†’ Local        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ’° Costo: $0/mes
ğŸ”’ Privacidad: 100% (sin envÃ­o de datos)
âš¡ Velocidad: Sin latencia de red
```

### AI Conversacional: FLEXIBLE âš™ï¸

**ConfiguraciÃ³n Actual (Default):**
- âœ… **Ollama Local** con modelo `phi3:mini` descargado en tu mÃ¡quina
- âœ… **Sin conexiÃ³n a internet** para generar respuestas
- âœ… **Privacidad total** - Conversaciones no salen de tu Mac

**Opciones Disponibles (Configurables):**

#### OpciÃ³n 1: Ollama Local (ACTUAL) âœ…
```typescript
// apps/backend/src/modules/ai/ollama.service.ts
OLLAMA_URL=http://chatbotdysa-ollama:11434
OLLAMA_MODEL=phi3:mini

Ventajas:
  âœ… Gratis ($0)
  âœ… Privado (datos no salen)
  âœ… RÃ¡pido (sin latencia red)
  âœ… Sin lÃ­mites de uso

Desventajas:
  âš ï¸ Calidad de respuestas menor que GPT-4/Claude
  âš ï¸ Consume recursos locales (RAM/CPU)
```

#### OpciÃ³n 2: OpenAI API (Configurable)
```typescript
// .env
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4-turbo-preview

Ventajas:
  âœ… Mejor calidad de respuestas
  âœ… No consume recursos locales
  âœ… Modelos mÃ¡s avanzados

Desventajas:
  âŒ Costo por uso (~$0.01-0.03 por request)
  âŒ Requiere internet
  âŒ Datos enviados a OpenAI
  âŒ LÃ­mites de rate (RPM)
```

#### OpciÃ³n 3: Anthropic Claude API (Configurable)
```typescript
// .env
ANTHROPIC_API_KEY=sk-ant-...
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

Ventajas:
  âœ… Excelente calidad de respuestas
  âœ… Contexto largo (200K tokens)
  âœ… Muy bueno para espaÃ±ol

Desventajas:
  âŒ Costo por uso (~$0.003-0.015 por request)
  âŒ Requiere internet
  âŒ Datos enviados a Anthropic
```

#### OpciÃ³n 4: Hybrid (Configurable)
```typescript
// Backend puede elegir automÃ¡ticamente
- Ollama local para queries simples
- API cloud para queries complejas
- Fallback a local si API falla

Archivo: apps/backend/src/modules/ai/hybrid-ai.service.ts
```

---

## ğŸ”§ CÃ“MO CAMBIAR LA CONFIGURACIÃ“N

### Mantener Todo Local (ACTUAL) âœ…

**No hacer nada.** Sistema ya configurado con Ollama local.

### Agregar OpenAI API (Opcional)

1. Obtener API key de OpenAI: https://platform.openai.com/api-keys

2. Agregar a `.env`:
```bash
OPENAI_API_KEY=sk-proj-...
OPENAI_MODEL=gpt-4-turbo-preview
USE_OPENAI=true
```

3. Reiniciar backend:
```bash
docker restart chatbotdysa-backend
```

### Agregar Anthropic Claude API (Opcional)

1. Obtener API key de Anthropic: https://console.anthropic.com/

2. Agregar a `.env`:
```bash
ANTHROPIC_API_KEY=sk-ant-api03-...
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
USE_ANTHROPIC=true
```

3. Reiniciar backend:
```bash
docker restart chatbotdysa-backend
```

### Modo Hybrid (Lo mejor de ambos)

```bash
# .env
USE_HYBRID=true
OLLAMA_URL=http://chatbotdysa-ollama:11434
OPENAI_API_KEY=sk-proj-...
ANTHROPIC_API_KEY=sk-ant-api03-...

# Estrategia
AI_STRATEGY=smart  # Usa local para simple, cloud para complejo
```

---

## ğŸ“Š COMPARACIÃ“N DE COSTOS

### Escenario: 1,000 conversaciones/mes

| OpciÃ³n | Costo Mensual | Calidad | Privacidad | Internet |
|--------|---------------|---------|------------|----------|
| **Ollama Local** | $0 | 7/10 | 100% | No requiere |
| **OpenAI GPT-3.5** | ~$10-20 | 8/10 | Datos a OpenAI | Requiere |
| **OpenAI GPT-4** | ~$30-50 | 9.5/10 | Datos a OpenAI | Requiere |
| **Claude 3.5 Sonnet** | ~$15-30 | 9/10 | Datos a Anthropic | Requiere |
| **Hybrid** | ~$5-15 | 8.5/10 | Parcial | Requiere |

---

## ğŸ”’ PRIVACIDAD Y DATOS

### Ollama Local (Actual)
```
Usuario â†’ Backend Local â†’ Ollama Local â†’ Respuesta
         â””â”€ PostgreSQL Local (guarda conversaciÃ³n)

âœ… NADA sale de tu Mac
âœ… Conversaciones 100% privadas
âœ… Sin tÃ©rminos de servicio de terceros
```

### APIs Cloud (Si se configura)
```
Usuario â†’ Backend Local â†’ API Cloud (OpenAI/Claude) â†’ Respuesta
         â””â”€ PostgreSQL Local (guarda conversaciÃ³n)

âš ï¸ Mensaje enviado a proveedor cloud
âš ï¸ Sujeto a tÃ©rminos de servicio
âš ï¸ Datos procesados en servidores externos
```

---

## ğŸ’¡ RECOMENDACIÃ“N

### Para Desarrollo/Testing (AHORA)
**Usar Ollama Local** âœ…
- Gratis
- RÃ¡pido para probar
- Sin dependencias externas
- Privacidad total

### Para ProducciÃ³n con Clientes Reales (FUTURO)
**Considerar Hybrid:**
- Ollama local para queries bÃ¡sicas (70% de casos)
- Claude/GPT-4 para queries complejas (30% de casos)
- Balance entre costo y calidad
- Fallback a local si API falla

### Para Clientes Enterprise (FUTURO)
**OpciÃ³n On-Premise:**
- Ollama con modelos mÃ¡s grandes (llama3:70b, mixtral:8x7b)
- Sin conexiÃ³n a internet
- Privacidad garantizada
- Puede requerir GPU dedicado

---

## ğŸš€ MODELOS OLLAMA DISPONIBLES

### Actuales en tu Sistema
```bash
# Verificar modelos instalados
docker exec chatbotdysa-ollama ollama list

# Actualmente tienes:
phi3:mini (2.4GB) âœ… - Instalado y funcionando
```

### Otros Modelos Disponibles para Descargar

**PequeÃ±os (para laptop):**
```bash
# Descargar modelos adicionales
docker exec chatbotdysa-ollama ollama pull llama3.2:3b  # 2GB
docker exec chatbotdysa-ollama ollama pull gemma2:2b    # 1.6GB
```

**Medianos (mejor calidad):**
```bash
docker exec chatbotdysa-ollama ollama pull llama3:8b    # 4.7GB
docker exec chatbotdysa-ollama ollama pull mistral:7b   # 4.1GB
```

**Grandes (requiere GPU/mucha RAM):**
```bash
docker exec chatbotdysa-ollama ollama pull llama3:70b   # 40GB
docker exec chatbotdysa-ollama ollama pull mixtral:8x7b # 26GB
```

**Cambiar modelo activo:**
```bash
# En .env
OLLAMA_MODEL=llama3:8b  # Cambiar de phi3:mini a llama3:8b
```

---

## ğŸ“ CONFIGURACIÃ“N ACTUAL

```bash
# apps/backend/.env (o variables Docker)

# AI Configuration
OLLAMA_URL=http://chatbotdysa-ollama:11434
OLLAMA_MODEL=phi3:mini

# Opcionales (comentadas por defecto)
# OPENAI_API_KEY=
# OPENAI_MODEL=gpt-4-turbo-preview
# ANTHROPIC_API_KEY=
# ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
# USE_HYBRID=false
```

---

## âœ… RESUMEN

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  CONFIGURACIÃ“N ACTUAL (RECOMENDADA PARA AHORA)          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Sistema Core:       100% Local âœ…
  - Backend         â†’ Local
  - Database        â†’ Local
  - Cache           â†’ Local
  - Frontend        â†’ Local

AI Conversacional:  Local (Ollama phi3:mini) âœ…
  - Modelo          â†’ phi3:mini (2.4GB)
  - UbicaciÃ³n       â†’ localhost:21434
  - Costo           â†’ $0/mes
  - Privacidad      â†’ 100%
  - Internet        â†’ No requiere

OpciÃ³n Futura:      APIs Cloud (OpenAI/Claude)
  - Estado          â†’ Disponible pero NO activado
  - ActivaciÃ³n      â†’ Manual (agregar API keys)
  - Uso             â†’ Solo si tÃº lo configuras
```

**El bot solo usarÃ¡ APIs externas si TÃš explÃ­citamente las configuras.
Por defecto, todo es 100% local.**

---

**Generado:** 28 de Octubre de 2025, 22:10 CLT
**Estado:** Sistema 100% Local con AI Local (Ollama)
